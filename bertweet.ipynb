{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1ef1f4",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad57550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, html, time, warnings, torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder, normalize, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical\n",
    "from sklearn.base import BaseEstimator\n",
    "from skopt.callbacks import DeltaYStopper\n",
    "from skopt.utils import point_asdict\n",
    "from collections import deque\n",
    "import umap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50782b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40314faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min_cleaned = pd.read_csv(\"min_cleaned_tweets.csv\")\n",
    "texts = df_min_cleaned[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "df_min_cleaned_sample = pd.read_csv(\"min_cleaned_sample_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fd4a8",
   "metadata": {},
   "source": [
    "# BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982d3a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentiment classes: ['negative', 'neutral', 'positive']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8787ac137c004bfaa9e59375bd333733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-24T07:37:21.318663Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x147791480>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.322651Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x147817600>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.355769Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x1478175c0>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.363697Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781f440>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.367843Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781ffc0>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.368118Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781fc80>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.368307Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781fa40>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.368526Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781f900>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.383147Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781f780>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m model_name = BERTWEET_MODEL\n\u001b[32m     24\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m model = \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4900\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4891\u001b[39m     gguf_file\n\u001b[32m   4892\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4893\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4894\u001b[39m ):\n\u001b[32m   4895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4896\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4897\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4898\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4900\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4902\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4907\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4913\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4920\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4921\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/transformers/modeling_utils.py:1066\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1063\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1064\u001b[39m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[32m   1065\u001b[39m         filename = _add_variant(WEIGHTS_NAME, variant)\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(WEIGHTS_NAME, variant):\n\u001b[32m   1070\u001b[39m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[32m   1071\u001b[39m     resolved_archive_file = cached_file(\n\u001b[32m   1072\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m   1073\u001b[39m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[32m   1074\u001b[39m         **cached_file_kwargs,\n\u001b[32m   1075\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1168\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1181\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1720\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[32m   1719\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1720\u001b[39m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1728\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants.HF_HUB_DISABLE_XET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:621\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    610\u001b[39m     displayed_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename[:\u001b[32m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(…)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    612\u001b[39m progress_cm = _get_progress_bar_context(\n\u001b[32m    613\u001b[39m     desc=displayed_filename,\n\u001b[32m    614\u001b[39m     log_level=logger.getEffectiveLevel(),\n\u001b[32m   (...)\u001b[39m\u001b[32m    618\u001b[39m     _tqdm_bar=_tqdm_bar,\n\u001b[32m    619\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_cm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mprogress_updater\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprogress_bytes\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/tqdm/std.py:1140\u001b[39m, in \u001b[36mtqdm.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m   1142\u001b[39m         \u001b[38;5;66;03m# maybe eager thread cleanup upon external error\u001b[39;00m\n\u001b[32m   1143\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (exc_type, exc_value, traceback) == (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/tqdm/notebook.py:275\u001b[39m, in \u001b[36mtqdm_notebook.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.disable:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Try to detect if there was an error or KeyboardInterrupt\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# in manual mode: if n < total, things probably got wrong\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n < \u001b[38;5;28mself\u001b[39m.total:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/tqdm/std.py:1275\u001b[39m, in \u001b[36mtqdm.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1273\u001b[39m \u001b[38;5;66;03m# decrement instance pos and remove from internal set\u001b[39;00m\n\u001b[32m   1274\u001b[39m pos = \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mself\u001b[39m.pos)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decr_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_print_t < \u001b[38;5;28mself\u001b[39m.start_t + \u001b[38;5;28mself\u001b[39m.delay:\n\u001b[32m   1278\u001b[39m     \u001b[38;5;66;03m# haven't ever displayed; nothing to clear\u001b[39;00m\n\u001b[32m   1279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/tqdm/std.py:696\u001b[39m, in \u001b[36mtqdm._decr_instances\u001b[39m\u001b[34m(cls, instance)\u001b[39m\n\u001b[32m    686\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decr_instances\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[32m    688\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    689\u001b[39m \u001b[33;03m    Remove from list and reposition another unfixed bar\u001b[39;00m\n\u001b[32m    690\u001b[39m \u001b[33;03m    to fill the new gap.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    694\u001b[39m \u001b[33;03m    (tqdm<=4.44.1 moved ALL subsequent unfixed bars up.)\u001b[39;00m\n\u001b[32m    695\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lock\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_instances\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/tqdm/std.py:111\u001b[39m, in \u001b[36mTqdmDefaultWriteLock.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Clustering-Sentiment-Analysis/venv/lib/python3.13/site-packages/tqdm/std.py:104\u001b[39m, in \u001b[36mTqdmDefaultWriteLock.acquire\u001b[39m\u001b[34m(self, *a, **k)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, *a, **k):\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m lock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.locks:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "BERTWEET_MODEL = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "\n",
    "# ==========================================================\n",
    "# 1️⃣ Load & Clean Datasets\n",
    "# ==========================================================\n",
    "\n",
    "# Encode sentiments consistently\n",
    "le = LabelEncoder()\n",
    "le.fit(df_min_cleaned[\"airline_sentiment\"].astype(str))\n",
    "df_min_cleaned[\"label_encoded\"] = le.transform(df_min_cleaned[\"airline_sentiment\"].astype(str))\n",
    "df_min_cleaned_sample[\"label_encoded\"] = le.transform(df_min_cleaned_sample[\"airline_sentiment\"].astype(str))\n",
    "print(f\"✅ Sentiment classes: {list(le.classes_)}\")\n",
    "\n",
    "texts_full = df_min_cleaned[\"clean_text\"].tolist()\n",
    "texts_sample = df_min_cleaned_sample[\"clean_text\"].tolist()\n",
    "y_full = df_min_cleaned[\"label_encoded\"].values\n",
    "y_sample = df_min_cleaned_sample[\"label_encoded\"].values\n",
    "\n",
    "# ==========================================================\n",
    "# 2️⃣ Load Pretrained Model (BERTweet)\n",
    "# ==========================================================\n",
    "model_name = BERTWEET_MODEL\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_hidden_states=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"✅ Model loaded on device: {device}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 3️⃣ Extract Embeddings + Logits\n",
    "# ==========================================================\n",
    "def get_logits(texts, batch_size=32):\n",
    "    all_logits = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting logits\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "        all_logits.append(outputs.logits.detach().cpu().numpy())\n",
    "    return np.vstack(all_logits)\n",
    "\n",
    "def get_hidden_embeddings(texts, batch_size=32, layer=-2):\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[layer]\n",
    "            all_embs.append(hidden_states.mean(dim=1).cpu().numpy())\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "print(\"\\n🔹 Extracting SAMPLE (20%) embeddings...\")\n",
    "X_emb_sample = get_hidden_embeddings(texts_sample)\n",
    "X_logits_sample = get_logits(texts_sample)\n",
    "X_hybrid_sample = np.concatenate([X_emb_sample, X_logits_sample], axis=1).astype(np.float64)\n",
    "print(f\"✅ SAMPLE hybrid features shape: {X_hybrid_sample.shape}\")\n",
    "\n",
    "print(\"\\n🔹 Extracting FULL dataset embeddings...\")\n",
    "X_emb_full = get_hidden_embeddings(texts_full)\n",
    "X_logits_full = get_logits(texts_full)\n",
    "X_hybrid_full = np.concatenate([X_emb_full, X_logits_full], axis=1).astype(np.float64)\n",
    "print(f\"✅ FULL hybrid features shape: {X_hybrid_full.shape}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 4️⃣ Clustering Optimization Setup\n",
    "# ==========================================================\n",
    "def hungarian_accuracy(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    r, c = linear_sum_assignment(-cm)\n",
    "    return cm[r, c].sum() / cm.sum()\n",
    "\n",
    "def evaluate_full(X, y_true_int, labels, sil_metric):\n",
    "    sil = silhouette_score(X, labels, metric=sil_metric)\n",
    "    ari = adjusted_rand_score(y_true_int, labels)\n",
    "    nmi = normalized_mutual_info_score(y_true_int, labels)\n",
    "    acc = hungarian_accuracy(y_true_int, labels)\n",
    "    return sil, ari, nmi, acc\n",
    "\n",
    "# ==========================================================\n",
    "# 5️⃣ PCA Scree Plot + UMAP Grids\n",
    "# ==========================================================\n",
    "def pick_pca_candidates(X, pct_low=0.80, pct_high=0.95, k=5, plot=True):\n",
    "    max_components = X.shape[1]\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=max_components, random_state=RANDOM_STATE)\n",
    "    pca.fit(Xs)\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    cum = np.cumsum(evr)\n",
    "    thresholds = np.linspace(pct_low, pct_high, k)\n",
    "    comps = [int(np.argmax(cum >= t)) + 1 for t in thresholds]\n",
    "    comps = sorted(set(comps))\n",
    "\n",
    "    if plot:\n",
    "        xs = np.arange(1, len(evr) + 1)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(xs, evr, marker=\"o\", label=\"Individual\")\n",
    "        plt.plot(xs, cum, marker=\"x\", label=\"Cumulative\")\n",
    "        for t, c in zip(thresholds, comps):\n",
    "            plt.axvline(c, linestyle=\"--\", alpha=0.3)\n",
    "            plt.text(c, 0.02, f\"{int(t*100)}%→{c}\", rotation=90, va=\"bottom\", ha=\"right\", fontsize=8)\n",
    "        plt.title(\"Scree Plot — Explained & Cumulative Variance (BERTweet Hybrid)\")\n",
    "        plt.xlabel(\"Principal Component\"); plt.ylabel(\"Variance Ratio\")\n",
    "        plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "    return comps, cum\n",
    "\n",
    "pca_candidates, _cum = pick_pca_candidates(X_hybrid_sample, pct_low=0.80, pct_high=0.95, k=5, plot=True)\n",
    "umap_neighbors  = [15, 30, 45, 60, 75, 100, 150, 200]\n",
    "umap_min_dist   = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "umap_components = [16, 32, 48, 64, 96, 128]\n",
    "\n",
    "# ==========================================================\n",
    "# 6️⃣ Clustering Pipeline + BayesSearchCV\n",
    "# ==========================================================\n",
    "_LAST_TRIAL = deque(maxlen=1)\n",
    "\n",
    "class ClusteringPipeline(BaseEstimator):\n",
    "    def __init__(self, reducer='pca', n_components=50,\n",
    "                 n_neighbors=15, min_dist=0.1, n_components_umap=16,\n",
    "                 model='kmeans', cov_type='full', linkage='average'):\n",
    "        self.reducer = reducer\n",
    "        self.n_components = n_components\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_dist = min_dist\n",
    "        self.n_components_umap = n_components_umap\n",
    "        self.model = model\n",
    "        self.cov_type = cov_type\n",
    "        self.linkage = linkage\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        _LAST_TRIAL.clear(); _LAST_TRIAL.append(self.__dict__)\n",
    "        if self.reducer == 'pca':\n",
    "            X_std = StandardScaler().fit_transform(X)\n",
    "            X_red = PCA(n_components=self.n_components, random_state=RANDOM_STATE).fit_transform(X_std)\n",
    "        else:\n",
    "            X_l2 = normalize(X)\n",
    "            X_red = umap.UMAP(\n",
    "                n_neighbors=self.n_neighbors,\n",
    "                min_dist=self.min_dist,\n",
    "                n_components=self.n_components_umap,\n",
    "                metric='cosine',\n",
    "                random_state=RANDOM_STATE\n",
    "            ).fit_transform(X_l2)\n",
    "\n",
    "        if self.model == 'kmeans':\n",
    "            X_use = normalize(X_red)\n",
    "            labels = KMeans(n_clusters=3, n_init=10, algorithm='elkan', random_state=RANDOM_STATE).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "        elif self.model == 'gmm':\n",
    "            X_use = np.asarray(X_red, dtype=np.float64)\n",
    "            try:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type\n",
    "                ).fit_predict(X_use)\n",
    "            except ValueError:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type, reg_covar=1e-5\n",
    "                ).fit_predict(X_use)\n",
    "            self.metric = \"euclidean\"\n",
    "        else:\n",
    "            X_use = normalize(X_red)\n",
    "            labels = AgglomerativeClustering(\n",
    "                n_clusters=3, linkage=self.linkage, metric='cosine'\n",
    "            ).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "\n",
    "        self.labels_ = labels\n",
    "        self.X_use_ = X_use\n",
    "        self.score_ = silhouette_score(X_use, labels, metric=self.metric)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return self.score_\n",
    "\n",
    "search_spaces = [\n",
    "    {'reducer': Categorical(['pca']),\n",
    "     'n_components': Categorical(pca_candidates),\n",
    "     'model': Categorical(['kmeans'])},\n",
    "    {'reducer': Categorical(['pca']),\n",
    "     'n_components': Categorical(pca_candidates),\n",
    "     'model': Categorical(['gmm']),\n",
    "     'cov_type': Categorical(['full','tied','diag','spherical'])},\n",
    "    {'reducer': Categorical(['pca']),\n",
    "     'n_components': Categorical(pca_candidates),\n",
    "     'model': Categorical(['agglo']),\n",
    "     'linkage': Categorical(['average','complete','single'])},\n",
    "    {'reducer': Categorical(['umap']),\n",
    "     'n_neighbors': Categorical(umap_neighbors),\n",
    "     'min_dist': Categorical(umap_min_dist),\n",
    "     'n_components_umap': Categorical(umap_components),\n",
    "     'model': Categorical(['kmeans'])},\n",
    "    {'reducer': Categorical(['umap']),\n",
    "     'n_neighbors': Categorical(umap_neighbors),\n",
    "     'min_dist': Categorical(umap_min_dist),\n",
    "     'n_components_umap': Categorical(umap_components),\n",
    "     'model': Categorical(['gmm']),\n",
    "     'cov_type': Categorical(['full','tied','diag','spherical'])},\n",
    "    {'reducer': Categorical(['umap']),\n",
    "     'n_neighbors': Categorical(umap_neighbors),\n",
    "     'min_dist': Categorical(umap_min_dist),\n",
    "     'n_components_umap': Categorical(umap_components),\n",
    "     'model': Categorical(['agglo']),\n",
    "     'linkage': Categorical(['average','complete','single'])},\n",
    "]\n",
    "\n",
    "TOTAL_ITERS = 60\n",
    "_timings, _start, _prev = [], [None], [None]\n",
    "\n",
    "def _infer_branch_from_space(space):\n",
    "    reducer = model = None\n",
    "    for dim in getattr(space, \"dimensions\", []):\n",
    "        cats = getattr(dim, \"categories\", None)\n",
    "        if cats and len(cats) == 1:\n",
    "            if cats[0] in (\"pca\", \"umap\"): reducer = cats[0]\n",
    "            elif cats[0] in (\"kmeans\", \"gmm\", \"agglo\"): model = cats[0]\n",
    "    return f\"{reducer or '?'}+{model or '?'}\"\n",
    "\n",
    "def progress_callback(res):\n",
    "    import numpy as np, time\n",
    "    now = time.perf_counter()\n",
    "    if _start[0] is None: _start[0] = now\n",
    "    if _prev[0] is not None: _timings.append(now - _prev[0])\n",
    "    k = len(res.x_iters)\n",
    "    avg = np.mean(_timings) if _timings else 0\n",
    "    elapsed = now - _start[0]; remaining = max(TOTAL_ITERS - k, 0) * avg\n",
    "    branch = _infer_branch_from_space(res.space)\n",
    "    print(f\"[Bayes] iter {k:>3}/{TOTAL_ITERS} ({k/TOTAL_ITERS:5.1%}) \"\n",
    "          f\"| avg {avg:5.2f}s | elapsed {elapsed/60:4.1f}m ETA {remaining/60:4.1f}m | branch {branch}\")\n",
    "    _prev[0] = now\n",
    "\n",
    "n = X_hybrid_sample.shape[0]\n",
    "dummy_y = np.zeros(n, dtype=int)\n",
    "cv_full = [(np.arange(n), np.arange(n))]\n",
    "\n",
    "search = BayesSearchCV(\n",
    "    estimator=ClusteringPipeline(),\n",
    "    search_spaces=search_spaces,\n",
    "    n_iter=TOTAL_ITERS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring=None,\n",
    "    cv=cv_full,\n",
    "    n_points=4,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== Running BayesSearchCV (PCA/UMAP × KMeans/GMM/Agglo) ===\")\n",
    "_prev[0] = time.perf_counter(); _start[0] = _prev[0]\n",
    "search.fit(X_hybrid_sample, dummy_y, callback=[DeltaYStopper(delta=1e-4, n_best=15), progress_callback])\n",
    "\n",
    "print(\"\\n🏁 DONE — Bayesian optimization complete.\")\n",
    "print(\"Best parameters:\", search.best_params_)\n",
    "print(\"Best score:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4335d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "EMB_DIM = 768\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ==========================================================\n",
    "# Load BERTweet embeddings (no normalization)\n",
    "# ==========================================================\n",
    "model_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_hidden_states=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"✅ Model loaded on device: {device}\")\n",
    "\n",
    "def get_logits(texts, batch_size=32):\n",
    "    all_logits = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting logits\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "        all_logits.append(outputs.logits.detach().cpu().numpy())\n",
    "    return np.vstack(all_logits)\n",
    "\n",
    "def get_hidden_embeddings(texts, batch_size=32, layer=-2):\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[layer]\n",
    "            all_embs.append(hidden_states.mean(dim=1).cpu().numpy())\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics\n",
    "# ---------------------------\n",
    "def hungarian_accuracy(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    r, c = linear_sum_assignment(-cm)\n",
    "    return cm[r, c].sum() / cm.sum()\n",
    "\n",
    "def evaluate_full(X, y_true_int, labels, sil_metric):\n",
    "    sil = silhouette_score(X, labels, metric=sil_metric)\n",
    "    ari = adjusted_rand_score(y_true_int, labels)\n",
    "    nmi = normalized_mutual_info_score(y_true_int, labels)\n",
    "    acc = hungarian_accuracy(y_true_int, labels)\n",
    "    return sil, ari, nmi, acc\n",
    "\n",
    "# ---------------------------\n",
    "# Prepare Data\n",
    "# ---------------------------\n",
    "texts_sample = df_min_cleaned_sample[\"clean_text\"].astype(str).tolist()\n",
    "texts_full   = df_min_cleaned[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "# Encode sentiments consistently\n",
    "le = LabelEncoder()\n",
    "le.fit(df_min_cleaned[\"airline_sentiment\"].astype(str))\n",
    "df_min_cleaned[\"label_encoded\"] = le.transform(df_min_cleaned[\"airline_sentiment\"].astype(str))\n",
    "df_min_cleaned_sample[\"label_encoded\"] = le.transform(df_min_cleaned_sample[\"airline_sentiment\"].astype(str))\n",
    "\n",
    "texts_full = df_min_cleaned[\"cleaned_text\"].tolist()\n",
    "texts_sample = df_min_cleaned_sample[\"cleaned_text\"].tolist()\n",
    "y_full = df_min_cleaned[\"label_encoded\"].values\n",
    "y_sample = df_min_cleaned_sample[\"label_encoded\"].values\n",
    "\n",
    "X_emb_sample = get_hidden_embeddings(texts_sample)\n",
    "X_logits_sample = get_logits(texts_sample)\n",
    "X_hybrid_sample = np.concatenate([X_emb_sample, X_logits_sample], axis=1).astype(np.float64)\n",
    "\n",
    "X_emb_full = get_hidden_embeddings(texts_full)\n",
    "X_logits_full = get_logits(texts_full)\n",
    "X_hybrid_full = np.concatenate([X_emb_full, X_logits_full], axis=1).astype(np.float64)\n",
    "\n",
    "# ---------------------------\n",
    "# PCA Scree Plot + pick 5 n_components (80–95% cum var, evenly spaced)\n",
    "# ---------------------------\n",
    "def pick_pca_candidates(X, pct_low=0.80, pct_high=0.95, k=5, max_components=None, plot=True):\n",
    "    if max_components is None:\n",
    "        max_components = min(EMB_DIM, X.shape[1])\n",
    "    Xs   = StandardScaler().fit_transform(X)\n",
    "    pca  = PCA(n_components=max_components, random_state=RANDOM_STATE)\n",
    "    pca.fit(Xs)\n",
    "    evr  = pca.explained_variance_ratio_\n",
    "    cum  = np.cumsum(evr)\n",
    "\n",
    "    thresholds = np.linspace(pct_low, pct_high, k)\n",
    "    comps = []\n",
    "    for t in thresholds:\n",
    "        idx = int(np.argmax(cum >= t)) + 1\n",
    "        comps.append(idx)\n",
    "\n",
    "    # dedupe while preserving order\n",
    "    seen = set(); comps_unique = []\n",
    "    for c in comps:\n",
    "        if c not in seen:\n",
    "            comps_unique.append(c); seen.add(c)\n",
    "\n",
    "    # ensure exactly k values (best-effort padding)\n",
    "    while len(comps_unique) < k:\n",
    "        step = max(1, (comps_unique[-1] - comps_unique[0]) // (k-1))\n",
    "        candidate = min(max_components, comps_unique[-1] + step)\n",
    "        if candidate not in seen:\n",
    "            comps_unique.append(candidate); seen.add(candidate)\n",
    "        else:\n",
    "            candidate = min(max_components, candidate+1)\n",
    "            if candidate not in seen:\n",
    "                comps_unique.append(candidate); seen.add(candidate)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    comps_unique = sorted(comps_unique)[:k]\n",
    "\n",
    "    if plot:\n",
    "        xs = np.arange(1, len(evr)+1)\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(xs, evr, marker='o', label='Individual')\n",
    "        plt.plot(xs, cum, marker='x', label='Cumulative')\n",
    "        for t, c in zip(thresholds, comps):\n",
    "            plt.axvline(c, linestyle='--', alpha=0.3)\n",
    "            plt.text(c, 0.02, f'{int(t*100)}%→{c}', rotation=90, va='bottom', ha='right', fontsize=8)\n",
    "        plt.title('Scree Plot — Explained & Cumulative Variance')\n",
    "        plt.xlabel('Principal Component'); plt.ylabel('Variance Ratio')\n",
    "        plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "    return comps_unique, cum\n",
    "\n",
    "pca_candidates, _cum = pick_pca_candidates(\n",
    "    X_hybrid_sample, pct_low=0.80, pct_high=0.95, k=5, max_components=EMB_DIM, plot=True\n",
    ")\n",
    "print(\"PCA n_components candidates (80–95% cum var):\", pca_candidates)\n",
    "\n",
    "# Fixed UMAP candidate grids (your choices)\n",
    "umap_neighbors  = [15, 30, 45, 60, 75, 100, 150, 200]\n",
    "umap_min_dist   = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "umap_components = [16, 32, 48, 64, 96, 128]\n",
    "\n",
    "# ---------------------------\n",
    "# ClusteringPipeline class (for BayesSearchCV)\n",
    "# ---------------------------\n",
    "_LAST_TRIAL = deque(maxlen=1)\n",
    "\n",
    "class ClusteringPipeline(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 reducer='pca', n_components=50,\n",
    "                 n_neighbors=15, min_dist=0.1, n_components_umap=16,\n",
    "                 model='kmeans', cov_type='full', linkage='average'):\n",
    "        self.reducer = reducer\n",
    "        self.n_components = n_components\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_dist = min_dist\n",
    "        self.n_components_umap = n_components_umap\n",
    "        self.model = model\n",
    "        self.cov_type = cov_type\n",
    "        self.linkage = linkage\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # record the params actually used (for robust logging)\n",
    "        _LAST_TRIAL.clear()\n",
    "        _LAST_TRIAL.append({\n",
    "            'reducer'          : self.reducer,\n",
    "            'n_components'     : self.n_components,\n",
    "            'n_neighbors'      : self.n_neighbors,\n",
    "            'min_dist'         : self.min_dist,\n",
    "            'n_components_umap': self.n_components_umap,\n",
    "            'model'            : self.model,\n",
    "            'cov_type'         : self.cov_type,\n",
    "            'linkage'          : self.linkage,\n",
    "        })\n",
    "        # ---------------- Reducer ----------------\n",
    "        if self.reducer == 'pca':\n",
    "            X_std = StandardScaler().fit_transform(X)\n",
    "            X_red = PCA(n_components=self.n_components, random_state=RANDOM_STATE).fit_transform(X_std)\n",
    "        else:\n",
    "            X_l2 = normalize(X)\n",
    "            X_red = umap.UMAP(\n",
    "                n_neighbors=self.n_neighbors,\n",
    "                min_dist=self.min_dist,\n",
    "                n_components=self.n_components_umap,\n",
    "                metric='cosine',\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_epochs=120,\n",
    "                low_memory=True\n",
    "            ).fit_transform(X_l2)\n",
    "\n",
    "        # ---------------- Clustering ----------------\n",
    "        if self.model == 'kmeans':\n",
    "            X_use = normalize(X_red)\n",
    "            labels = KMeans(n_clusters=3, n_init=10, algorithm='elkan', random_state=RANDOM_STATE).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "        elif self.model == 'gmm':\n",
    "            X_use = np.asarray(X_red, dtype=np.float64)\n",
    "            try:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type\n",
    "                ).fit_predict(X_use)\n",
    "            except ValueError:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type, reg_covar=1e-5\n",
    "                ).fit_predict(X_use)\n",
    "            self.metric = \"euclidean\"\n",
    "        else:  # Agglo\n",
    "            X_use = normalize(X_red)\n",
    "            labels = AgglomerativeClustering(\n",
    "                n_clusters=3, linkage=self.linkage, metric='cosine'\n",
    "            ).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "\n",
    "        # store for score()\n",
    "        self.labels_ = labels\n",
    "        self.X_use_  = X_use\n",
    "        self.score_  = silhouette_score(X_use, labels, metric=self.metric)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return self.score_\n",
    "\n",
    "# ---------------------------\n",
    "# Bayesian Search Space \n",
    "# ---------------------------\n",
    "search_spaces = [\n",
    "    # PCA + KMeans\n",
    "    {\n",
    "        'reducer'      : Categorical(['pca'],        name='reducer'),\n",
    "        'n_components' : Categorical(pca_candidates, name='n_components'),\n",
    "        'model'        : Categorical(['kmeans'],     name='model'),\n",
    "    },\n",
    "    # PCA + GMM\n",
    "    {\n",
    "        'reducer'      : Categorical(['pca'],        name='reducer'),\n",
    "        'n_components' : Categorical(pca_candidates, name='n_components'),\n",
    "        'model'        : Categorical(['gmm'],        name='model'),\n",
    "        'cov_type'     : Categorical(['full','tied','diag','spherical'], name='cov_type'),\n",
    "    },\n",
    "    # PCA + Agglo (no 'ward' since metric='cosine')\n",
    "    {\n",
    "        'reducer'      : Categorical(['pca'],        name='reducer'),\n",
    "        'n_components' : Categorical(pca_candidates, name='n_components'),\n",
    "        'model'        : Categorical(['agglo'],      name='model'),\n",
    "        'linkage'      : Categorical(['average','complete','single'], name='linkage'),\n",
    "    },\n",
    "\n",
    "    # UMAP + KMeans\n",
    "    {\n",
    "        'reducer'           : Categorical(['umap'],  name='reducer'),\n",
    "        'n_neighbors'       : Categorical(umap_neighbors,  name='n_neighbors'),\n",
    "        'min_dist'          : Categorical(umap_min_dist,   name='min_dist'),\n",
    "        'n_components_umap' : Categorical(umap_components, name='n_components_umap'),\n",
    "        'model'             : Categorical(['kmeans'],      name='model'),\n",
    "    },\n",
    "    # UMAP + GMM\n",
    "    {\n",
    "        'reducer'           : Categorical(['umap'],  name='reducer'),\n",
    "        'n_neighbors'       : Categorical(umap_neighbors,  name='n_neighbors'),\n",
    "        'min_dist'          : Categorical(umap_min_dist,   name='min_dist'),\n",
    "        'n_components_umap' : Categorical(umap_components, name='n_components_umap'),\n",
    "        'model'             : Categorical(['gmm'],         name='model'),\n",
    "        'cov_type'          : Categorical(['full','tied','diag','spherical'], name='cov_type'),\n",
    "    },\n",
    "    # UMAP + Agglo\n",
    "    {\n",
    "        'reducer'           : Categorical(['umap'],  name='reducer'),\n",
    "        'n_neighbors'       : Categorical(umap_neighbors,  name='n_neighbors'),\n",
    "        'min_dist'          : Categorical(umap_min_dist,   name='min_dist'),\n",
    "        'n_components_umap' : Categorical(umap_components, name='n_components_umap'),\n",
    "        'model'             : Categorical(['agglo'],       name='model'),\n",
    "        'linkage'           : Categorical(['average','complete','single'], name='linkage'),\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- progress/timing callback for BayesSearchCV (with robust fallback) ---\n",
    "TOTAL_ITERS = 60  # keep in sync with BayesSearchCV(n_iter=...)\n",
    "\n",
    "_timings = []\n",
    "_start = [None]\n",
    "_prev  = [None]\n",
    "\n",
    "def _short_params(d):\n",
    "    keys = [\n",
    "        'reducer','n_components','n_neighbors','min_dist','n_components_umap',\n",
    "        'model','cov_type','linkage'\n",
    "    ]\n",
    "    return {k: d[k] for k in keys if k in d}\n",
    "\n",
    "def _infer_branch_from_space(space):\n",
    "    \"\"\"Infer branch name (reducer+model) from single-choice categories in subspace.\"\"\"\n",
    "    reducer = model = None\n",
    "    for dim in getattr(space, \"dimensions\", []):\n",
    "        cats = getattr(dim, \"categories\", None)\n",
    "        if not cats or not hasattr(cats, \"__iter__\"):\n",
    "            continue\n",
    "        if len(cats) == 1:\n",
    "            v = cats[0]\n",
    "            if v in (\"pca\", \"umap\"):\n",
    "                reducer = v\n",
    "            elif v in (\"kmeans\", \"gmm\", \"agglo\"):\n",
    "                model = v\n",
    "    return f\"{reducer or '?'}+{model or '?'}\"\n",
    "\n",
    "def progress_callback(res):\n",
    "    import time, numpy as np\n",
    "    now = time.perf_counter()\n",
    "    if _start[0] is None:\n",
    "        _start[0] = now\n",
    "    if _prev[0] is not None:\n",
    "        _timings.append(now - _prev[0])\n",
    "\n",
    "    k = len(res.x_iters)\n",
    "    avg = float(np.mean(_timings)) if _timings else 0.0\n",
    "    elapsed = now - _start[0]\n",
    "    remaining = max(TOTAL_ITERS - k, 0) * (avg if avg > 0 else 0.0)\n",
    "\n",
    "    # Try to get the reducer/model from the latest evaluated point\n",
    "    branch = None\n",
    "    try:\n",
    "        if res.x_iters:\n",
    "            last_params = point_asdict(res.space, res.x_iters[-1])\n",
    "            r, m = last_params.get(\"reducer\"), last_params.get(\"model\")\n",
    "            if r and m:\n",
    "                branch = f\"{r}+{m}\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: infer directly from this subspace definition\n",
    "    if branch is None:\n",
    "        branch = _infer_branch_from_space(res.space)\n",
    "\n",
    "    last_dt = _timings[-1] if _timings else 0.0\n",
    "    print(\n",
    "        f\"[Bayes] iter {k:>3}/{TOTAL_ITERS} ({k/TOTAL_ITERS:5.1%}) \"\n",
    "        f\"| last {last_dt:5.2f}s avg {avg:5.2f}s \"\n",
    "        f\"| elapsed {elapsed/60:4.1f}m ETA ~{remaining/60:4.1f}m \"\n",
    "        f\"| branch {branch}\",\n",
    "        flush=True\n",
    "    )\n",
    "    _prev[0] = now\n",
    "    return False\n",
    "\n",
    "# ---------------------------\n",
    "# Run Bayesian SearchCV\n",
    "# ---------------------------\n",
    "n = X_hybrid_sample.shape[0]\n",
    "dummy_y = np.zeros(n, dtype=int)\n",
    "cv_full = [(np.arange(n), np.arange(n))]\n",
    "\n",
    "search = BayesSearchCV(\n",
    "    estimator=ClusteringPipeline(),\n",
    "    search_spaces=search_spaces,           # <-- your original list-of-branches\n",
    "    n_iter=TOTAL_ITERS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring=None,                          # uses estimator.score() (silhouette)\n",
    "    cv=cv_full,\n",
    "    n_points=4,                            # parallel proposals\n",
    "    n_jobs=-1,                             # parallel fits\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== Running BayesSearchCV (PCA/UMAP × KMeans/GMM/Agglo) ===\")\n",
    "callbacks = [\n",
    "    DeltaYStopper(delta=1e-4, n_best=15),\n",
    "    progress_callback\n",
    "]\n",
    "\n",
    "# prime timers for clean first measurement\n",
    "_prev[0] = time.perf_counter()\n",
    "_start[0] = _prev[0]\n",
    "\n",
    "search.fit(X_hybrid_sample, dummy_y, callback=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48828f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "\n",
    "# ==========================================================\n",
    "# Final (Full-Data) ClusteringPipeline — no speed tweaks\n",
    "# ==========================================================\n",
    "class ClusteringPipeline(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 reducer='pca', n_components=50,\n",
    "                 n_neighbors=15, min_dist=0.1, n_components_umap=16,\n",
    "                 model='kmeans', cov_type='full', linkage='average'):\n",
    "        self.reducer = reducer\n",
    "        self.n_components = n_components\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_dist = min_dist\n",
    "        self.n_components_umap = n_components_umap\n",
    "        self.model = model\n",
    "        self.cov_type = cov_type\n",
    "        self.linkage = linkage\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.reducer == 'pca':\n",
    "            X_std = StandardScaler().fit_transform(X)\n",
    "            X_red = PCA(n_components=self.n_components, random_state=RANDOM_STATE).fit_transform(X_std)\n",
    "        else:\n",
    "            X_l2 = normalize(X)\n",
    "            X_red = umap.UMAP(\n",
    "                n_neighbors=self.n_neighbors,\n",
    "                min_dist=self.min_dist,\n",
    "                n_components=self.n_components_umap,\n",
    "                metric='cosine',\n",
    "                random_state=RANDOM_STATE\n",
    "            ).fit_transform(X_l2)\n",
    "\n",
    "        if self.model == 'kmeans':\n",
    "            X_use = normalize(X_red)\n",
    "            labels = KMeans(n_clusters=3, n_init=10, algorithm='elkan',\n",
    "                            random_state=RANDOM_STATE).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "        elif self.model == 'gmm':\n",
    "            X_use = np.asarray(X_red, dtype=np.float64)\n",
    "            try:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type\n",
    "                ).fit_predict(X_use)\n",
    "            except ValueError:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type, reg_covar=1e-5\n",
    "                ).fit_predict(X_use)\n",
    "            self.metric = \"euclidean\"\n",
    "        else:  # Agglomerative\n",
    "            X_use = normalize(X_red)\n",
    "            labels = AgglomerativeClustering(\n",
    "                n_clusters=3, linkage=self.linkage, metric='cosine'\n",
    "            ).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "\n",
    "        self.labels_ = labels\n",
    "        self.X_use_ = X_use\n",
    "        self.score_ = silhouette_score(X_use, labels, metric=self.metric)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return self.score_\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 1️⃣  Extract best per-branch on 20% subset\n",
    "# ==========================================================\n",
    "def _params_compact(d):\n",
    "    order = [\"reducer\",\"model\",\"n_components\",\n",
    "             \"n_neighbors\",\"min_dist\",\"n_components_umap\",\n",
    "             \"cov_type\",\"linkage\"]\n",
    "    return \", \".join(f\"{k}={d[k]}\" for k in order if k in d and pd.notnull(d[k]))\n",
    "\n",
    "cv = pd.DataFrame(search.cv_results_)\n",
    "param_cols = [c for c in cv.columns if c.startswith(\"param_\")]\n",
    "score_col  = \"mean_test_score\"  # silhouette\n",
    "\n",
    "# Ensure plain Python types\n",
    "for c in param_cols:\n",
    "    cv[c] = cv[c].apply(lambda x: x if isinstance(x, (str,int,float,type(None))) else str(x))\n",
    "\n",
    "cv[\"branch\"] = cv.apply(lambda r: f\"{r.get('param_reducer','?')}+{r.get('param_model','?')}\", axis=1)\n",
    "best_idx = cv.groupby(\"branch\")[score_col].idxmax()\n",
    "best_rows = cv.loc[best_idx].reset_index(drop=True)\n",
    "\n",
    "subset_records = []\n",
    "for _, r in best_rows.iterrows():\n",
    "    params = {p.replace(\"param_\",\"\"): r[p] for p in param_cols if pd.notnull(r[p])}\n",
    "    subset_records.append({\n",
    "        \"branch\": r[\"branch\"],\n",
    "        \"best_sil_subset\": round(float(r[score_col]), 4),\n",
    "        **params\n",
    "    })\n",
    "\n",
    "df_best_subset = pd.DataFrame(subset_records).sort_values(\"branch\")\n",
    "\n",
    "print(\"\\n================ Best Parameters per Branch (20% subset) ================\\n\")\n",
    "print(df_best_subset[[\n",
    "    \"branch\",\"best_sil_subset\",\"reducer\",\"model\",\"n_components\",\n",
    "    \"n_neighbors\",\"min_dist\",\"n_components_umap\",\"cov_type\",\"linkage\"\n",
    "]].to_string(index=False))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 2️⃣  Refit each best branch on FULL data + compute metrics\n",
    "# ==========================================================\n",
    "final_rows = []\n",
    "for _, row in df_best_subset.iterrows():\n",
    "    params = {}\n",
    "    for k in [\"reducer\",\"model\",\"n_components\",\"n_neighbors\",\"min_dist\",\n",
    "              \"n_components_umap\",\"cov_type\",\"linkage\"]:\n",
    "        if k in row and pd.notnull(row[k]):\n",
    "            val = row[k]\n",
    "            if k in [\"n_components\",\"n_neighbors\",\"n_components_umap\"] and not pd.isna(val):\n",
    "                val = int(val)\n",
    "            if k == \"min_dist\" and not pd.isna(val):\n",
    "                val = float(val)\n",
    "            params[k] = val\n",
    "\n",
    "    mdl = ClusteringPipeline(**params)\n",
    "    mdl.fit(X_glove_full)\n",
    "    labels = mdl.labels_\n",
    "    metric = getattr(mdl, \"metric\", \"cosine\")\n",
    "    sil, ari, nmi, acc = evaluate_full(mdl.X_use_, y_full, labels, metric)\n",
    "\n",
    "    final_rows.append({\n",
    "        \"branch\": row[\"branch\"],\n",
    "        \"Silhouette\": round(sil, 3),\n",
    "        \"ARI\": round(ari, 3),\n",
    "        \"NMI\": round(nmi, 3),\n",
    "        \"Hungarian\": round(acc, 3),\n",
    "        \"Params\": _params_compact(params)\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(final_rows).sort_values(\"branch\")\n",
    "\n",
    "print(\"\\n================ Final Evaluation on Full Minimal-Cleaned Data (All 6 best models) ================\\n\")\n",
    "print(df_final[[\"branch\",\"Silhouette\",\"ARI\",\"NMI\",\"Hungarian\",\"Params\"]].to_string(index=False))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3️⃣  Quick metric winners\n",
    "# ==========================================================\n",
    "for metric in [\"Silhouette\",\"ARI\",\"NMI\",\"Hungarian\"]:\n",
    "    r = df_final.loc[df_final[metric].idxmax()]\n",
    "    print(f\"\\nWinner by {metric}: {r['branch']} | {metric}={r[metric]:.3f} | {r['Params']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# UMAP viz of best 6 branches\n",
    "# ===========================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_CLUSTERS = 3  # airline sentiment: neg / neu / pos\n",
    "\n",
    "def fit_reduce(X, reducer, params):\n",
    "    if reducer == \"pca\":\n",
    "        X_std = StandardScaler().fit_transform(X)\n",
    "        X_red = PCA(n_components=params[\"n_components\"], random_state=RANDOM_STATE).fit_transform(X_std)\n",
    "    elif reducer == \"umap\":\n",
    "        X_l2 = normalize(X)  # cosine-friendly\n",
    "        X_red = umap.UMAP(\n",
    "            n_neighbors=params[\"n_neighbors\"],\n",
    "            min_dist=params[\"min_dist\"],\n",
    "            n_components=params[\"n_components_umap\"],\n",
    "            metric=\"cosine\",\n",
    "            random_state=RANDOM_STATE\n",
    "        ).fit_transform(X_l2)\n",
    "    else:\n",
    "        X_red = X\n",
    "    return X_red\n",
    "\n",
    "def fit_cluster(X_red, model, params):\n",
    "    if model == \"kmeans\":\n",
    "        X_use = normalize(X_red)\n",
    "        labels = KMeans(n_clusters=N_CLUSTERS, n_init=10, algorithm=\"elkan\",\n",
    "                        random_state=RANDOM_STATE).fit_predict(X_use)\n",
    "    elif model == \"gmm\":\n",
    "        X_use = np.asarray(X_red, dtype=np.float64)\n",
    "        try:\n",
    "            labels = GaussianMixture(\n",
    "                n_components=N_CLUSTERS, n_init=10, random_state=RANDOM_STATE,\n",
    "                covariance_type=params[\"cov_type\"]\n",
    "            ).fit_predict(X_use)\n",
    "        except ValueError:\n",
    "            labels = GaussianMixture(\n",
    "                n_components=N_CLUSTERS, n_init=10, random_state=RANDOM_STATE,\n",
    "                covariance_type=params[\"cov_type\"], reg_covar=1e-5\n",
    "            ).fit_predict(X_use)\n",
    "    elif model == \"agglo\":\n",
    "        X_use = normalize(X_red)\n",
    "        labels = AgglomerativeClustering(\n",
    "            n_clusters=N_CLUSTERS, linkage=params[\"linkage\"], metric=\"cosine\"\n",
    "        ).fit_predict(X_use)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    return labels\n",
    "\n",
    "# Your tuned params:\n",
    "models_params = {\n",
    "    \"pca+agglo\": dict(reducer=\"pca\",  model=\"agglo\",  n_components=27, linkage=\"complete\"),\n",
    "    \"pca+gmm\":   dict(reducer=\"pca\",  model=\"gmm\",    n_components=27, cov_type=\"spherical\"),\n",
    "    \"pca+kmeans\":dict(reducer=\"pca\",  model=\"kmeans\", n_components=27),\n",
    "    \"umap+agglo\":dict(reducer=\"umap\", model=\"agglo\",  n_neighbors=200, min_dist=0.1, n_components_umap=16, linkage=\"average\"),\n",
    "    \"umap+gmm\":  dict(reducer=\"umap\", model=\"gmm\",    n_neighbors=200, min_dist=0.1, n_components_umap=48, cov_type=\"spherical\"),\n",
    "    \"umap+kmeans\":dict(reducer=\"umap\",model=\"kmeans\", n_neighbors=150, min_dist=0.1, n_components_umap=16),\n",
    "}\n",
    "\n",
    "# Build 2D UMAP per branch for visualization (re-fitted for each branch's feature space)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, prm) in enumerate(models_params.items()):\n",
    "    # 1) branch-specific reduction\n",
    "    X_red = fit_reduce(X_hybrid_full, prm[\"reducer\"], prm)\n",
    "    # 2) clustering with your pipeline conventions\n",
    "    labels = fit_cluster(X_red, prm[\"model\"], prm)\n",
    "    # 3) 2D UMAP for visualization\n",
    "    umap_2d = umap.UMAP(n_neighbors=50, min_dist=0.1, n_components=2,\n",
    "                        metric=\"euclidean\", random_state=RANDOM_STATE)\n",
    "    X_vis = umap_2d.fit_transform(X_red)\n",
    "\n",
    "    ax = axes[i]\n",
    "    sc = ax.scatter(X_vis[:, 0], X_vis[:, 1], c=labels, s=5, cmap=\"Spectral\")\n",
    "    ax.set_title(name, fontsize=12)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "plt.suptitle(\"UMAP Visualization — Best Models on FULL BERTweet Hybrid Features\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
