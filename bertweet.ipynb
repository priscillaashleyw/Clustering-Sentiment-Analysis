{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1ef1f4",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad57550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, html, time, warnings, torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder, normalize, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical\n",
    "from sklearn.base import BaseEstimator\n",
    "from skopt.callbacks import DeltaYStopper\n",
    "from skopt.utils import point_asdict\n",
    "from collections import deque\n",
    "import umap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50782b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40314faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min_cleaned = pd.read_csv(\"min_cleaned_tweets.csv\")\n",
    "texts = df_min_cleaned[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "df_min_cleaned_sample = pd.read_csv(\"min_cleaned_sample_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971fd4a8",
   "metadata": {},
   "source": [
    "# BERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d3a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentiment classes: ['negative', 'neutral', 'positive']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8787ac137c004bfaa9e59375bd333733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-24T07:37:21.318663Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x147791480>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.322651Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x147817600>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.355769Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x1478175c0>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.363697Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781f440>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.367843Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781ffc0>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.368118Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781fc80>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.368307Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781fa40>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.368526Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781f900>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n",
      "  \u001b[2m2025-10-24T07:37:21.383147Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0x120843fb0>), traceback: Some(<traceback object at 0x14781f780>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /Users/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "BERTWEET_MODEL = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "\n",
    "# ==========================================================\n",
    "# 1️⃣ Load & Clean Datasets\n",
    "# ==========================================================\n",
    "\n",
    "# Encode sentiments consistently\n",
    "le = LabelEncoder()\n",
    "le.fit(df_min_cleaned[\"airline_sentiment\"].astype(str))\n",
    "df_min_cleaned[\"label_encoded\"] = le.transform(df_min_cleaned[\"airline_sentiment\"].astype(str))\n",
    "df_min_cleaned_sample[\"label_encoded\"] = le.transform(df_min_cleaned_sample[\"airline_sentiment\"].astype(str))\n",
    "print(f\"✅ Sentiment classes: {list(le.classes_)}\")\n",
    "\n",
    "texts_full = df_min_cleaned[\"clean_text\"].tolist()\n",
    "texts_sample = df_min_cleaned_sample[\"clean_text\"].tolist()\n",
    "y_full = df_min_cleaned[\"label_encoded\"].values\n",
    "y_sample = df_min_cleaned_sample[\"label_encoded\"].values\n",
    "\n",
    "# ==========================================================\n",
    "# 2️⃣ Load Pretrained Model (BERTweet)\n",
    "# ==========================================================\n",
    "model_name = BERTWEET_MODEL\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_hidden_states=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"✅ Model loaded on device: {device}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 3️⃣ Extract Embeddings + Logits\n",
    "# ==========================================================\n",
    "def get_logits(texts, batch_size=32):\n",
    "    all_logits = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting logits\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "        all_logits.append(outputs.logits.detach().cpu().numpy())\n",
    "    return np.vstack(all_logits)\n",
    "\n",
    "def get_hidden_embeddings(texts, batch_size=32, layer=-2):\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[layer]\n",
    "            all_embs.append(hidden_states.mean(dim=1).cpu().numpy())\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "print(\"\\n🔹 Extracting SAMPLE (20%) embeddings...\")\n",
    "X_emb_sample = get_hidden_embeddings(texts_sample)\n",
    "X_logits_sample = get_logits(texts_sample)\n",
    "X_hybrid_sample = np.concatenate([X_emb_sample, X_logits_sample], axis=1).astype(np.float64)\n",
    "print(f\"✅ SAMPLE hybrid features shape: {X_hybrid_sample.shape}\")\n",
    "\n",
    "print(\"\\n🔹 Extracting FULL dataset embeddings...\")\n",
    "X_emb_full = get_hidden_embeddings(texts_full)\n",
    "X_logits_full = get_logits(texts_full)\n",
    "X_hybrid_full = np.concatenate([X_emb_full, X_logits_full], axis=1).astype(np.float64)\n",
    "print(f\"✅ FULL hybrid features shape: {X_hybrid_full.shape}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 4️⃣ Clustering Optimization Setup\n",
    "# ==========================================================\n",
    "def hungarian_accuracy(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    r, c = linear_sum_assignment(-cm)\n",
    "    return cm[r, c].sum() / cm.sum()\n",
    "\n",
    "def evaluate_full(X, y_true_int, labels, sil_metric):\n",
    "    sil = silhouette_score(X, labels, metric=sil_metric)\n",
    "    ari = adjusted_rand_score(y_true_int, labels)\n",
    "    nmi = normalized_mutual_info_score(y_true_int, labels)\n",
    "    acc = hungarian_accuracy(y_true_int, labels)\n",
    "    return sil, ari, nmi, acc\n",
    "\n",
    "# ==========================================================\n",
    "# 5️⃣ PCA Scree Plot + UMAP Grids\n",
    "# ==========================================================\n",
    "def pick_pca_candidates(X, pct_low=0.80, pct_high=0.95, k=5, plot=True):\n",
    "    max_components = X.shape[1]\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=max_components, random_state=RANDOM_STATE)\n",
    "    pca.fit(Xs)\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    cum = np.cumsum(evr)\n",
    "    thresholds = np.linspace(pct_low, pct_high, k)\n",
    "    comps = [int(np.argmax(cum >= t)) + 1 for t in thresholds]\n",
    "    comps = sorted(set(comps))\n",
    "\n",
    "    if plot:\n",
    "        xs = np.arange(1, len(evr) + 1)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(xs, evr, marker=\"o\", label=\"Individual\")\n",
    "        plt.plot(xs, cum, marker=\"x\", label=\"Cumulative\")\n",
    "        for t, c in zip(thresholds, comps):\n",
    "            plt.axvline(c, linestyle=\"--\", alpha=0.3)\n",
    "            plt.text(c, 0.02, f\"{int(t*100)}%→{c}\", rotation=90, va=\"bottom\", ha=\"right\", fontsize=8)\n",
    "        plt.title(\"Scree Plot — Explained & Cumulative Variance (BERTweet Hybrid)\")\n",
    "        plt.xlabel(\"Principal Component\"); plt.ylabel(\"Variance Ratio\")\n",
    "        plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "    return comps, cum\n",
    "\n",
    "pca_candidates, _cum = pick_pca_candidates(X_hybrid_sample, pct_low=0.80, pct_high=0.95, k=5, plot=True)\n",
    "umap_neighbors  = [15, 30, 45, 60, 75, 100, 150, 200]\n",
    "umap_min_dist   = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "umap_components = [16, 32, 48, 64, 96, 128]\n",
    "\n",
    "# ==========================================================\n",
    "# 6️⃣ Clustering Pipeline + BayesSearchCV\n",
    "# ==========================================================\n",
    "_LAST_TRIAL = deque(maxlen=1)\n",
    "\n",
    "class ClusteringPipeline(BaseEstimator):\n",
    "    def __init__(self, reducer='pca', n_components=50,\n",
    "                 n_neighbors=15, min_dist=0.1, n_components_umap=16,\n",
    "                 model='kmeans', cov_type='full', linkage='average'):\n",
    "        self.reducer = reducer\n",
    "        self.n_components = n_components\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_dist = min_dist\n",
    "        self.n_components_umap = n_components_umap\n",
    "        self.model = model\n",
    "        self.cov_type = cov_type\n",
    "        self.linkage = linkage\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        _LAST_TRIAL.clear(); _LAST_TRIAL.append(self.__dict__)\n",
    "        if self.reducer == 'pca':\n",
    "            X_std = StandardScaler().fit_transform(X)\n",
    "            X_red = PCA(n_components=self.n_components, random_state=RANDOM_STATE).fit_transform(X_std)\n",
    "        else:\n",
    "            X_l2 = normalize(X)\n",
    "            X_red = umap.UMAP(\n",
    "                n_neighbors=self.n_neighbors,\n",
    "                min_dist=self.min_dist,\n",
    "                n_components=self.n_components_umap,\n",
    "                metric='cosine',\n",
    "                random_state=RANDOM_STATE\n",
    "            ).fit_transform(X_l2)\n",
    "\n",
    "        if self.model == 'kmeans':\n",
    "            X_use = normalize(X_red)\n",
    "            labels = KMeans(n_clusters=3, n_init=10, algorithm='elkan', random_state=RANDOM_STATE).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "        elif self.model == 'gmm':\n",
    "            X_use = np.asarray(X_red, dtype=np.float64)\n",
    "            try:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type\n",
    "                ).fit_predict(X_use)\n",
    "            except ValueError:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type, reg_covar=1e-5\n",
    "                ).fit_predict(X_use)\n",
    "            self.metric = \"euclidean\"\n",
    "        else:\n",
    "            X_use = normalize(X_red)\n",
    "            labels = AgglomerativeClustering(\n",
    "                n_clusters=3, linkage=self.linkage, metric='cosine'\n",
    "            ).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "\n",
    "        self.labels_ = labels\n",
    "        self.X_use_ = X_use\n",
    "        self.score_ = silhouette_score(X_use, labels, metric=self.metric)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return self.score_\n",
    "\n",
    "search_spaces = [\n",
    "    {'reducer': Categorical(['pca']),\n",
    "     'n_components': Categorical(pca_candidates),\n",
    "     'model': Categorical(['kmeans'])},\n",
    "    {'reducer': Categorical(['pca']),\n",
    "     'n_components': Categorical(pca_candidates),\n",
    "     'model': Categorical(['gmm']),\n",
    "     'cov_type': Categorical(['full','tied','diag','spherical'])},\n",
    "    {'reducer': Categorical(['pca']),\n",
    "     'n_components': Categorical(pca_candidates),\n",
    "     'model': Categorical(['agglo']),\n",
    "     'linkage': Categorical(['average','complete','single'])},\n",
    "    {'reducer': Categorical(['umap']),\n",
    "     'n_neighbors': Categorical(umap_neighbors),\n",
    "     'min_dist': Categorical(umap_min_dist),\n",
    "     'n_components_umap': Categorical(umap_components),\n",
    "     'model': Categorical(['kmeans'])},\n",
    "    {'reducer': Categorical(['umap']),\n",
    "     'n_neighbors': Categorical(umap_neighbors),\n",
    "     'min_dist': Categorical(umap_min_dist),\n",
    "     'n_components_umap': Categorical(umap_components),\n",
    "     'model': Categorical(['gmm']),\n",
    "     'cov_type': Categorical(['full','tied','diag','spherical'])},\n",
    "    {'reducer': Categorical(['umap']),\n",
    "     'n_neighbors': Categorical(umap_neighbors),\n",
    "     'min_dist': Categorical(umap_min_dist),\n",
    "     'n_components_umap': Categorical(umap_components),\n",
    "     'model': Categorical(['agglo']),\n",
    "     'linkage': Categorical(['average','complete','single'])},\n",
    "]\n",
    "\n",
    "TOTAL_ITERS = 60\n",
    "_timings, _start, _prev = [], [None], [None]\n",
    "\n",
    "def _infer_branch_from_space(space):\n",
    "    reducer = model = None\n",
    "    for dim in getattr(space, \"dimensions\", []):\n",
    "        cats = getattr(dim, \"categories\", None)\n",
    "        if cats and len(cats) == 1:\n",
    "            if cats[0] in (\"pca\", \"umap\"): reducer = cats[0]\n",
    "            elif cats[0] in (\"kmeans\", \"gmm\", \"agglo\"): model = cats[0]\n",
    "    return f\"{reducer or '?'}+{model or '?'}\"\n",
    "\n",
    "def progress_callback(res):\n",
    "    import numpy as np, time\n",
    "    now = time.perf_counter()\n",
    "    if _start[0] is None: _start[0] = now\n",
    "    if _prev[0] is not None: _timings.append(now - _prev[0])\n",
    "    k = len(res.x_iters)\n",
    "    avg = np.mean(_timings) if _timings else 0\n",
    "    elapsed = now - _start[0]; remaining = max(TOTAL_ITERS - k, 0) * avg\n",
    "    branch = _infer_branch_from_space(res.space)\n",
    "    print(f\"[Bayes] iter {k:>3}/{TOTAL_ITERS} ({k/TOTAL_ITERS:5.1%}) \"\n",
    "          f\"| avg {avg:5.2f}s | elapsed {elapsed/60:4.1f}m ETA {remaining/60:4.1f}m | branch {branch}\")\n",
    "    _prev[0] = now\n",
    "\n",
    "n = X_hybrid_sample.shape[0]\n",
    "dummy_y = np.zeros(n, dtype=int)\n",
    "cv_full = [(np.arange(n), np.arange(n))]\n",
    "\n",
    "search = BayesSearchCV(\n",
    "    estimator=ClusteringPipeline(),\n",
    "    search_spaces=search_spaces,\n",
    "    n_iter=TOTAL_ITERS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring=None,\n",
    "    cv=cv_full,\n",
    "    n_points=4,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== Running BayesSearchCV (PCA/UMAP × KMeans/GMM/Agglo) ===\")\n",
    "_prev[0] = time.perf_counter(); _start[0] = _prev[0]\n",
    "search.fit(X_hybrid_sample, dummy_y, callback=[DeltaYStopper(delta=1e-4, n_best=15), progress_callback])\n",
    "\n",
    "print(\"\\n🏁 DONE — Bayesian optimization complete.\")\n",
    "print(\"Best parameters:\", search.best_params_)\n",
    "print(\"Best score:\", search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4335d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "EMB_DIM = 768\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ==========================================================\n",
    "# Load BERTweet embeddings (no normalization)\n",
    "# ==========================================================\n",
    "model_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_hidden_states=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"✅ Model loaded on device: {device}\")\n",
    "\n",
    "def get_logits(texts, batch_size=32):\n",
    "    all_logits = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting logits\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "        all_logits.append(outputs.logits.detach().cpu().numpy())\n",
    "    return np.vstack(all_logits)\n",
    "\n",
    "def get_hidden_embeddings(texts, batch_size=32, layer=-2):\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Extracting embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[layer]\n",
    "            all_embs.append(hidden_states.mean(dim=1).cpu().numpy())\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics\n",
    "# ---------------------------\n",
    "def hungarian_accuracy(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    r, c = linear_sum_assignment(-cm)\n",
    "    return cm[r, c].sum() / cm.sum()\n",
    "\n",
    "def evaluate_full(X, y_true_int, labels, sil_metric):\n",
    "    sil = silhouette_score(X, labels, metric=sil_metric)\n",
    "    ari = adjusted_rand_score(y_true_int, labels)\n",
    "    nmi = normalized_mutual_info_score(y_true_int, labels)\n",
    "    acc = hungarian_accuracy(y_true_int, labels)\n",
    "    return sil, ari, nmi, acc\n",
    "\n",
    "# ---------------------------\n",
    "# Prepare Data\n",
    "# ---------------------------\n",
    "texts_sample = df_min_cleaned_sample[\"clean_text\"].astype(str).tolist()\n",
    "texts_full   = df_min_cleaned[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "# Encode sentiments consistently\n",
    "le = LabelEncoder()\n",
    "le.fit(df_min_cleaned[\"airline_sentiment\"].astype(str))\n",
    "df_min_cleaned[\"label_encoded\"] = le.transform(df_min_cleaned[\"airline_sentiment\"].astype(str))\n",
    "df_min_cleaned_sample[\"label_encoded\"] = le.transform(df_min_cleaned_sample[\"airline_sentiment\"].astype(str))\n",
    "\n",
    "texts_full = df_min_cleaned[\"cleaned_text\"].tolist()\n",
    "texts_sample = df_min_cleaned_sample[\"cleaned_text\"].tolist()\n",
    "y_full = df_min_cleaned[\"label_encoded\"].values\n",
    "y_sample = df_min_cleaned_sample[\"label_encoded\"].values\n",
    "\n",
    "X_emb_sample = get_hidden_embeddings(texts_sample)\n",
    "X_logits_sample = get_logits(texts_sample)\n",
    "X_hybrid_sample = np.concatenate([X_emb_sample, X_logits_sample], axis=1).astype(np.float64)\n",
    "\n",
    "X_emb_full = get_hidden_embeddings(texts_full)\n",
    "X_logits_full = get_logits(texts_full)\n",
    "X_hybrid_full = np.concatenate([X_emb_full, X_logits_full], axis=1).astype(np.float64)\n",
    "\n",
    "# ---------------------------\n",
    "# PCA Scree Plot + pick 5 n_components (80–95% cum var, evenly spaced)\n",
    "# ---------------------------\n",
    "def pick_pca_candidates(X, pct_low=0.80, pct_high=0.95, k=5, max_components=None, plot=True):\n",
    "    if max_components is None:\n",
    "        max_components = min(EMB_DIM, X.shape[1])\n",
    "    Xs   = StandardScaler().fit_transform(X)\n",
    "    pca  = PCA(n_components=max_components, random_state=RANDOM_STATE)\n",
    "    pca.fit(Xs)\n",
    "    evr  = pca.explained_variance_ratio_\n",
    "    cum  = np.cumsum(evr)\n",
    "\n",
    "    thresholds = np.linspace(pct_low, pct_high, k)\n",
    "    comps = []\n",
    "    for t in thresholds:\n",
    "        idx = int(np.argmax(cum >= t)) + 1\n",
    "        comps.append(idx)\n",
    "\n",
    "    # dedupe while preserving order\n",
    "    seen = set(); comps_unique = []\n",
    "    for c in comps:\n",
    "        if c not in seen:\n",
    "            comps_unique.append(c); seen.add(c)\n",
    "\n",
    "    # ensure exactly k values (best-effort padding)\n",
    "    while len(comps_unique) < k:\n",
    "        step = max(1, (comps_unique[-1] - comps_unique[0]) // (k-1))\n",
    "        candidate = min(max_components, comps_unique[-1] + step)\n",
    "        if candidate not in seen:\n",
    "            comps_unique.append(candidate); seen.add(candidate)\n",
    "        else:\n",
    "            candidate = min(max_components, candidate+1)\n",
    "            if candidate not in seen:\n",
    "                comps_unique.append(candidate); seen.add(candidate)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    comps_unique = sorted(comps_unique)[:k]\n",
    "\n",
    "    if plot:\n",
    "        xs = np.arange(1, len(evr)+1)\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(xs, evr, marker='o', label='Individual')\n",
    "        plt.plot(xs, cum, marker='x', label='Cumulative')\n",
    "        for t, c in zip(thresholds, comps):\n",
    "            plt.axvline(c, linestyle='--', alpha=0.3)\n",
    "            plt.text(c, 0.02, f'{int(t*100)}%→{c}', rotation=90, va='bottom', ha='right', fontsize=8)\n",
    "        plt.title('Scree Plot — Explained & Cumulative Variance')\n",
    "        plt.xlabel('Principal Component'); plt.ylabel('Variance Ratio')\n",
    "        plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "    return comps_unique, cum\n",
    "\n",
    "pca_candidates, _cum = pick_pca_candidates(\n",
    "    X_hybrid_sample, pct_low=0.80, pct_high=0.95, k=5, max_components=EMB_DIM, plot=True\n",
    ")\n",
    "print(\"PCA n_components candidates (80–95% cum var):\", pca_candidates)\n",
    "\n",
    "# Fixed UMAP candidate grids (your choices)\n",
    "umap_neighbors  = [15, 30, 45, 60, 75, 100, 150, 200]\n",
    "umap_min_dist   = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "umap_components = [16, 32, 48, 64, 96, 128]\n",
    "\n",
    "# ---------------------------\n",
    "# ClusteringPipeline class (for BayesSearchCV)\n",
    "# ---------------------------\n",
    "_LAST_TRIAL = deque(maxlen=1)\n",
    "\n",
    "class ClusteringPipeline(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 reducer='pca', n_components=50,\n",
    "                 n_neighbors=15, min_dist=0.1, n_components_umap=16,\n",
    "                 model='kmeans', cov_type='full', linkage='average'):\n",
    "        self.reducer = reducer\n",
    "        self.n_components = n_components\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_dist = min_dist\n",
    "        self.n_components_umap = n_components_umap\n",
    "        self.model = model\n",
    "        self.cov_type = cov_type\n",
    "        self.linkage = linkage\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # record the params actually used (for robust logging)\n",
    "        _LAST_TRIAL.clear()\n",
    "        _LAST_TRIAL.append({\n",
    "            'reducer'          : self.reducer,\n",
    "            'n_components'     : self.n_components,\n",
    "            'n_neighbors'      : self.n_neighbors,\n",
    "            'min_dist'         : self.min_dist,\n",
    "            'n_components_umap': self.n_components_umap,\n",
    "            'model'            : self.model,\n",
    "            'cov_type'         : self.cov_type,\n",
    "            'linkage'          : self.linkage,\n",
    "        })\n",
    "        # ---------------- Reducer ----------------\n",
    "        if self.reducer == 'pca':\n",
    "            X_std = StandardScaler().fit_transform(X)\n",
    "            X_red = PCA(n_components=self.n_components, random_state=RANDOM_STATE).fit_transform(X_std)\n",
    "        else:\n",
    "            X_l2 = normalize(X)\n",
    "            X_red = umap.UMAP(\n",
    "                n_neighbors=self.n_neighbors,\n",
    "                min_dist=self.min_dist,\n",
    "                n_components=self.n_components_umap,\n",
    "                metric='cosine',\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_epochs=120,\n",
    "                low_memory=True\n",
    "            ).fit_transform(X_l2)\n",
    "\n",
    "        # ---------------- Clustering ----------------\n",
    "        if self.model == 'kmeans':\n",
    "            X_use = normalize(X_red)\n",
    "            labels = KMeans(n_clusters=3, n_init=10, algorithm='elkan', random_state=RANDOM_STATE).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "        elif self.model == 'gmm':\n",
    "            X_use = np.asarray(X_red, dtype=np.float64)\n",
    "            try:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type\n",
    "                ).fit_predict(X_use)\n",
    "            except ValueError:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type, reg_covar=1e-5\n",
    "                ).fit_predict(X_use)\n",
    "            self.metric = \"euclidean\"\n",
    "        else:  # Agglo\n",
    "            X_use = normalize(X_red)\n",
    "            labels = AgglomerativeClustering(\n",
    "                n_clusters=3, linkage=self.linkage, metric='cosine'\n",
    "            ).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "\n",
    "        # store for score()\n",
    "        self.labels_ = labels\n",
    "        self.X_use_  = X_use\n",
    "        self.score_  = silhouette_score(X_use, labels, metric=self.metric)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return self.score_\n",
    "\n",
    "# ---------------------------\n",
    "# Bayesian Search Space \n",
    "# ---------------------------\n",
    "search_spaces = [\n",
    "    # PCA + KMeans\n",
    "    {\n",
    "        'reducer'      : Categorical(['pca'],        name='reducer'),\n",
    "        'n_components' : Categorical(pca_candidates, name='n_components'),\n",
    "        'model'        : Categorical(['kmeans'],     name='model'),\n",
    "    },\n",
    "    # PCA + GMM\n",
    "    {\n",
    "        'reducer'      : Categorical(['pca'],        name='reducer'),\n",
    "        'n_components' : Categorical(pca_candidates, name='n_components'),\n",
    "        'model'        : Categorical(['gmm'],        name='model'),\n",
    "        'cov_type'     : Categorical(['full','tied','diag','spherical'], name='cov_type'),\n",
    "    },\n",
    "    # PCA + Agglo (no 'ward' since metric='cosine')\n",
    "    {\n",
    "        'reducer'      : Categorical(['pca'],        name='reducer'),\n",
    "        'n_components' : Categorical(pca_candidates, name='n_components'),\n",
    "        'model'        : Categorical(['agglo'],      name='model'),\n",
    "        'linkage'      : Categorical(['average','complete','single'], name='linkage'),\n",
    "    },\n",
    "\n",
    "    # UMAP + KMeans\n",
    "    {\n",
    "        'reducer'           : Categorical(['umap'],  name='reducer'),\n",
    "        'n_neighbors'       : Categorical(umap_neighbors,  name='n_neighbors'),\n",
    "        'min_dist'          : Categorical(umap_min_dist,   name='min_dist'),\n",
    "        'n_components_umap' : Categorical(umap_components, name='n_components_umap'),\n",
    "        'model'             : Categorical(['kmeans'],      name='model'),\n",
    "    },\n",
    "    # UMAP + GMM\n",
    "    {\n",
    "        'reducer'           : Categorical(['umap'],  name='reducer'),\n",
    "        'n_neighbors'       : Categorical(umap_neighbors,  name='n_neighbors'),\n",
    "        'min_dist'          : Categorical(umap_min_dist,   name='min_dist'),\n",
    "        'n_components_umap' : Categorical(umap_components, name='n_components_umap'),\n",
    "        'model'             : Categorical(['gmm'],         name='model'),\n",
    "        'cov_type'          : Categorical(['full','tied','diag','spherical'], name='cov_type'),\n",
    "    },\n",
    "    # UMAP + Agglo\n",
    "    {\n",
    "        'reducer'           : Categorical(['umap'],  name='reducer'),\n",
    "        'n_neighbors'       : Categorical(umap_neighbors,  name='n_neighbors'),\n",
    "        'min_dist'          : Categorical(umap_min_dist,   name='min_dist'),\n",
    "        'n_components_umap' : Categorical(umap_components, name='n_components_umap'),\n",
    "        'model'             : Categorical(['agglo'],       name='model'),\n",
    "        'linkage'           : Categorical(['average','complete','single'], name='linkage'),\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- progress/timing callback for BayesSearchCV (with robust fallback) ---\n",
    "TOTAL_ITERS = 60  # keep in sync with BayesSearchCV(n_iter=...)\n",
    "\n",
    "_timings = []\n",
    "_start = [None]\n",
    "_prev  = [None]\n",
    "\n",
    "def _short_params(d):\n",
    "    keys = [\n",
    "        'reducer','n_components','n_neighbors','min_dist','n_components_umap',\n",
    "        'model','cov_type','linkage'\n",
    "    ]\n",
    "    return {k: d[k] for k in keys if k in d}\n",
    "\n",
    "def _infer_branch_from_space(space):\n",
    "    \"\"\"Infer branch name (reducer+model) from single-choice categories in subspace.\"\"\"\n",
    "    reducer = model = None\n",
    "    for dim in getattr(space, \"dimensions\", []):\n",
    "        cats = getattr(dim, \"categories\", None)\n",
    "        if not cats or not hasattr(cats, \"__iter__\"):\n",
    "            continue\n",
    "        if len(cats) == 1:\n",
    "            v = cats[0]\n",
    "            if v in (\"pca\", \"umap\"):\n",
    "                reducer = v\n",
    "            elif v in (\"kmeans\", \"gmm\", \"agglo\"):\n",
    "                model = v\n",
    "    return f\"{reducer or '?'}+{model or '?'}\"\n",
    "\n",
    "def progress_callback(res):\n",
    "    import time, numpy as np\n",
    "    now = time.perf_counter()\n",
    "    if _start[0] is None:\n",
    "        _start[0] = now\n",
    "    if _prev[0] is not None:\n",
    "        _timings.append(now - _prev[0])\n",
    "\n",
    "    k = len(res.x_iters)\n",
    "    avg = float(np.mean(_timings)) if _timings else 0.0\n",
    "    elapsed = now - _start[0]\n",
    "    remaining = max(TOTAL_ITERS - k, 0) * (avg if avg > 0 else 0.0)\n",
    "\n",
    "    # Try to get the reducer/model from the latest evaluated point\n",
    "    branch = None\n",
    "    try:\n",
    "        if res.x_iters:\n",
    "            last_params = point_asdict(res.space, res.x_iters[-1])\n",
    "            r, m = last_params.get(\"reducer\"), last_params.get(\"model\")\n",
    "            if r and m:\n",
    "                branch = f\"{r}+{m}\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: infer directly from this subspace definition\n",
    "    if branch is None:\n",
    "        branch = _infer_branch_from_space(res.space)\n",
    "\n",
    "    last_dt = _timings[-1] if _timings else 0.0\n",
    "    print(\n",
    "        f\"[Bayes] iter {k:>3}/{TOTAL_ITERS} ({k/TOTAL_ITERS:5.1%}) \"\n",
    "        f\"| last {last_dt:5.2f}s avg {avg:5.2f}s \"\n",
    "        f\"| elapsed {elapsed/60:4.1f}m ETA ~{remaining/60:4.1f}m \"\n",
    "        f\"| branch {branch}\",\n",
    "        flush=True\n",
    "    )\n",
    "    _prev[0] = now\n",
    "    return False\n",
    "\n",
    "# ---------------------------\n",
    "# Run Bayesian SearchCV\n",
    "# ---------------------------\n",
    "n = X_hybrid_sample.shape[0]\n",
    "dummy_y = np.zeros(n, dtype=int)\n",
    "cv_full = [(np.arange(n), np.arange(n))]\n",
    "\n",
    "search = BayesSearchCV(\n",
    "    estimator=ClusteringPipeline(),\n",
    "    search_spaces=search_spaces,           # <-- your original list-of-branches\n",
    "    n_iter=TOTAL_ITERS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring=None,                          # uses estimator.score() (silhouette)\n",
    "    cv=cv_full,\n",
    "    n_points=4,                            # parallel proposals\n",
    "    n_jobs=-1,                             # parallel fits\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"\\n=== Running BayesSearchCV (PCA/UMAP × KMeans/GMM/Agglo) ===\")\n",
    "callbacks = [\n",
    "    DeltaYStopper(delta=1e-4, n_best=15),\n",
    "    progress_callback\n",
    "]\n",
    "\n",
    "# prime timers for clean first measurement\n",
    "_prev[0] = time.perf_counter()\n",
    "_start[0] = _prev[0]\n",
    "\n",
    "search.fit(X_hybrid_sample, dummy_y, callback=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48828f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "\n",
    "# ==========================================================\n",
    "# Final (Full-Data) ClusteringPipeline — no speed tweaks\n",
    "# ==========================================================\n",
    "class ClusteringPipeline(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 reducer='pca', n_components=50,\n",
    "                 n_neighbors=15, min_dist=0.1, n_components_umap=16,\n",
    "                 model='kmeans', cov_type='full', linkage='average'):\n",
    "        self.reducer = reducer\n",
    "        self.n_components = n_components\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_dist = min_dist\n",
    "        self.n_components_umap = n_components_umap\n",
    "        self.model = model\n",
    "        self.cov_type = cov_type\n",
    "        self.linkage = linkage\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.reducer == 'pca':\n",
    "            X_std = StandardScaler().fit_transform(X)\n",
    "            X_red = PCA(n_components=self.n_components, random_state=RANDOM_STATE).fit_transform(X_std)\n",
    "        else:\n",
    "            X_l2 = normalize(X)\n",
    "            X_red = umap.UMAP(\n",
    "                n_neighbors=self.n_neighbors,\n",
    "                min_dist=self.min_dist,\n",
    "                n_components=self.n_components_umap,\n",
    "                metric='cosine',\n",
    "                random_state=RANDOM_STATE\n",
    "            ).fit_transform(X_l2)\n",
    "\n",
    "        if self.model == 'kmeans':\n",
    "            X_use = normalize(X_red)\n",
    "            labels = KMeans(n_clusters=3, n_init=10, algorithm='elkan',\n",
    "                            random_state=RANDOM_STATE).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "        elif self.model == 'gmm':\n",
    "            X_use = np.asarray(X_red, dtype=np.float64)\n",
    "            try:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type\n",
    "                ).fit_predict(X_use)\n",
    "            except ValueError:\n",
    "                labels = GaussianMixture(\n",
    "                    n_components=3, n_init=10, random_state=RANDOM_STATE,\n",
    "                    covariance_type=self.cov_type, reg_covar=1e-5\n",
    "                ).fit_predict(X_use)\n",
    "            self.metric = \"euclidean\"\n",
    "        else:  # Agglomerative\n",
    "            X_use = normalize(X_red)\n",
    "            labels = AgglomerativeClustering(\n",
    "                n_clusters=3, linkage=self.linkage, metric='cosine'\n",
    "            ).fit_predict(X_use)\n",
    "            self.metric = \"cosine\"\n",
    "\n",
    "        self.labels_ = labels\n",
    "        self.X_use_ = X_use\n",
    "        self.score_ = silhouette_score(X_use, labels, metric=self.metric)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        return self.score_\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 1️⃣  Extract best per-branch on 20% subset\n",
    "# ==========================================================\n",
    "def _params_compact(d):\n",
    "    order = [\"reducer\",\"model\",\"n_components\",\n",
    "             \"n_neighbors\",\"min_dist\",\"n_components_umap\",\n",
    "             \"cov_type\",\"linkage\"]\n",
    "    return \", \".join(f\"{k}={d[k]}\" for k in order if k in d and pd.notnull(d[k]))\n",
    "\n",
    "cv = pd.DataFrame(search.cv_results_)\n",
    "param_cols = [c for c in cv.columns if c.startswith(\"param_\")]\n",
    "score_col  = \"mean_test_score\"  # silhouette\n",
    "\n",
    "# Ensure plain Python types\n",
    "for c in param_cols:\n",
    "    cv[c] = cv[c].apply(lambda x: x if isinstance(x, (str,int,float,type(None))) else str(x))\n",
    "\n",
    "cv[\"branch\"] = cv.apply(lambda r: f\"{r.get('param_reducer','?')}+{r.get('param_model','?')}\", axis=1)\n",
    "best_idx = cv.groupby(\"branch\")[score_col].idxmax()\n",
    "best_rows = cv.loc[best_idx].reset_index(drop=True)\n",
    "\n",
    "subset_records = []\n",
    "for _, r in best_rows.iterrows():\n",
    "    params = {p.replace(\"param_\",\"\"): r[p] for p in param_cols if pd.notnull(r[p])}\n",
    "    subset_records.append({\n",
    "        \"branch\": r[\"branch\"],\n",
    "        \"best_sil_subset\": round(float(r[score_col]), 4),\n",
    "        **params\n",
    "    })\n",
    "\n",
    "df_best_subset = pd.DataFrame(subset_records).sort_values(\"branch\")\n",
    "\n",
    "print(\"\\n================ Best Parameters per Branch (20% subset) ================\\n\")\n",
    "print(df_best_subset[[\n",
    "    \"branch\",\"best_sil_subset\",\"reducer\",\"model\",\"n_components\",\n",
    "    \"n_neighbors\",\"min_dist\",\"n_components_umap\",\"cov_type\",\"linkage\"\n",
    "]].to_string(index=False))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 2️⃣  Refit each best branch on FULL data + compute metrics\n",
    "# ==========================================================\n",
    "final_rows = []\n",
    "for _, row in df_best_subset.iterrows():\n",
    "    params = {}\n",
    "    for k in [\"reducer\",\"model\",\"n_components\",\"n_neighbors\",\"min_dist\",\n",
    "              \"n_components_umap\",\"cov_type\",\"linkage\"]:\n",
    "        if k in row and pd.notnull(row[k]):\n",
    "            val = row[k]\n",
    "            if k in [\"n_components\",\"n_neighbors\",\"n_components_umap\"] and not pd.isna(val):\n",
    "                val = int(val)\n",
    "            if k == \"min_dist\" and not pd.isna(val):\n",
    "                val = float(val)\n",
    "            params[k] = val\n",
    "\n",
    "    mdl = ClusteringPipeline(**params)\n",
    "    mdl.fit(X_glove_full)\n",
    "    labels = mdl.labels_\n",
    "    metric = getattr(mdl, \"metric\", \"cosine\")\n",
    "    sil, ari, nmi, acc = evaluate_full(mdl.X_use_, y_full, labels, metric)\n",
    "\n",
    "    final_rows.append({\n",
    "        \"branch\": row[\"branch\"],\n",
    "        \"Silhouette\": round(sil, 3),\n",
    "        \"ARI\": round(ari, 3),\n",
    "        \"NMI\": round(nmi, 3),\n",
    "        \"Hungarian\": round(acc, 3),\n",
    "        \"Params\": _params_compact(params)\n",
    "    })\n",
    "\n",
    "df_final = pd.DataFrame(final_rows).sort_values(\"branch\")\n",
    "\n",
    "print(\"\\n================ Final Evaluation on Full Minimal-Cleaned Data (All 6 best models) ================\\n\")\n",
    "print(df_final[[\"branch\",\"Silhouette\",\"ARI\",\"NMI\",\"Hungarian\",\"Params\"]].to_string(index=False))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3️⃣  Quick metric winners\n",
    "# ==========================================================\n",
    "for metric in [\"Silhouette\",\"ARI\",\"NMI\",\"Hungarian\"]:\n",
    "    r = df_final.loc[df_final[metric].idxmax()]\n",
    "    print(f\"\\nWinner by {metric}: {r['branch']} | {metric}={r[metric]:.3f} | {r['Params']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# UMAP viz of best 6 branches\n",
    "# ===========================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_CLUSTERS = 3  # airline sentiment: neg / neu / pos\n",
    "\n",
    "def fit_reduce(X, reducer, params):\n",
    "    if reducer == \"pca\":\n",
    "        X_std = StandardScaler().fit_transform(X)\n",
    "        X_red = PCA(n_components=params[\"n_components\"], random_state=RANDOM_STATE).fit_transform(X_std)\n",
    "    elif reducer == \"umap\":\n",
    "        X_l2 = normalize(X)  # cosine-friendly\n",
    "        X_red = umap.UMAP(\n",
    "            n_neighbors=params[\"n_neighbors\"],\n",
    "            min_dist=params[\"min_dist\"],\n",
    "            n_components=params[\"n_components_umap\"],\n",
    "            metric=\"cosine\",\n",
    "            random_state=RANDOM_STATE\n",
    "        ).fit_transform(X_l2)\n",
    "    else:\n",
    "        X_red = X\n",
    "    return X_red\n",
    "\n",
    "def fit_cluster(X_red, model, params):\n",
    "    if model == \"kmeans\":\n",
    "        X_use = normalize(X_red)\n",
    "        labels = KMeans(n_clusters=N_CLUSTERS, n_init=10, algorithm=\"elkan\",\n",
    "                        random_state=RANDOM_STATE).fit_predict(X_use)\n",
    "    elif model == \"gmm\":\n",
    "        X_use = np.asarray(X_red, dtype=np.float64)\n",
    "        try:\n",
    "            labels = GaussianMixture(\n",
    "                n_components=N_CLUSTERS, n_init=10, random_state=RANDOM_STATE,\n",
    "                covariance_type=params[\"cov_type\"]\n",
    "            ).fit_predict(X_use)\n",
    "        except ValueError:\n",
    "            labels = GaussianMixture(\n",
    "                n_components=N_CLUSTERS, n_init=10, random_state=RANDOM_STATE,\n",
    "                covariance_type=params[\"cov_type\"], reg_covar=1e-5\n",
    "            ).fit_predict(X_use)\n",
    "    elif model == \"agglo\":\n",
    "        X_use = normalize(X_red)\n",
    "        labels = AgglomerativeClustering(\n",
    "            n_clusters=N_CLUSTERS, linkage=params[\"linkage\"], metric=\"cosine\"\n",
    "        ).fit_predict(X_use)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "    return labels\n",
    "\n",
    "# Your tuned params:\n",
    "models_params = {\n",
    "    \"pca+agglo\": dict(reducer=\"pca\",  model=\"agglo\",  n_components=27, linkage=\"complete\"),\n",
    "    \"pca+gmm\":   dict(reducer=\"pca\",  model=\"gmm\",    n_components=27, cov_type=\"spherical\"),\n",
    "    \"pca+kmeans\":dict(reducer=\"pca\",  model=\"kmeans\", n_components=27),\n",
    "    \"umap+agglo\":dict(reducer=\"umap\", model=\"agglo\",  n_neighbors=200, min_dist=0.1, n_components_umap=16, linkage=\"average\"),\n",
    "    \"umap+gmm\":  dict(reducer=\"umap\", model=\"gmm\",    n_neighbors=200, min_dist=0.1, n_components_umap=48, cov_type=\"spherical\"),\n",
    "    \"umap+kmeans\":dict(reducer=\"umap\",model=\"kmeans\", n_neighbors=150, min_dist=0.1, n_components_umap=16),\n",
    "}\n",
    "\n",
    "# Build 2D UMAP per branch for visualization (re-fitted for each branch's feature space)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, prm) in enumerate(models_params.items()):\n",
    "    # 1) branch-specific reduction\n",
    "    X_red = fit_reduce(X_hybrid_full, prm[\"reducer\"], prm)\n",
    "    # 2) clustering with your pipeline conventions\n",
    "    labels = fit_cluster(X_red, prm[\"model\"], prm)\n",
    "    # 3) 2D UMAP for visualization\n",
    "    umap_2d = umap.UMAP(n_neighbors=50, min_dist=0.1, n_components=2,\n",
    "                        metric=\"euclidean\", random_state=RANDOM_STATE)\n",
    "    X_vis = umap_2d.fit_transform(X_red)\n",
    "\n",
    "    ax = axes[i]\n",
    "    sc = ax.scatter(X_vis[:, 0], X_vis[:, 1], c=labels, s=5, cmap=\"Spectral\")\n",
    "    ax.set_title(name, fontsize=12)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "plt.suptitle(\"UMAP Visualization — Best Models on FULL BERTweet Hybrid Features\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
