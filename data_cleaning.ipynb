{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c12a9c",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a4baaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "import contractions\n",
    "import html\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary NLTK data is available\n",
    "for pkg in ['punkt', 'averaged_perceptron_tagger', 'wordnet', 'stopwords']:\n",
    "    nltk.download(pkg, quiet=True)\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb588c1",
   "metadata": {},
   "source": [
    "# Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60bff87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.6559</td>\n",
       "      <td>@virginamerica Well, I didn'tâ€¦but NOW I DO! :-D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  sentiment_confidence  \\\n",
       "0           neutral                1.0000   \n",
       "1          positive                0.3486   \n",
       "2           neutral                0.6837   \n",
       "3          negative                1.0000   \n",
       "4          negative                1.0000   \n",
       "5          negative                1.0000   \n",
       "6          positive                0.6745   \n",
       "7           neutral                0.6340   \n",
       "8          positive                0.6559   \n",
       "9          positive                1.0000   \n",
       "\n",
       "                                                text  \n",
       "0                @VirginAmerica What @dhepburn said.  \n",
       "1  @VirginAmerica plus you've added commercials t...  \n",
       "2  @VirginAmerica I didn't today... Must mean I n...  \n",
       "3  @VirginAmerica it's really aggressive to blast...  \n",
       "4  @VirginAmerica and it's a really big bad thing...  \n",
       "5  @VirginAmerica seriously would pay $30 a fligh...  \n",
       "6  @VirginAmerica yes, nearly every time I fly VX...  \n",
       "7  @VirginAmerica Really missed a prime opportuni...  \n",
       "8    @virginamerica Well, I didn'tâ€¦but NOW I DO! :-D  \n",
       "9  @VirginAmerica it was amazing, and arrived an ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv', encoding='utf-8-sig')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55c322",
   "metadata": {},
   "source": [
    "# Fully Cleaned Data \n",
    "## (Traditional Features Extraction, TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afd1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIONS = {\n",
    "    \"no\",\"nor\",\"not\",\"never\",\"cannot\",\"cant\",\"can't\",\n",
    "    \"do not\",\"does not\",\"did not\",\n",
    "    \"is not\",\"are not\",\"was not\",\"were not\",\n",
    "    \"will not\",\"won't\",\"wont\",\n",
    "    \"would not\",\"should not\",\"could not\",\n",
    "    \"have not\",\"has not\",\"had not\",\n",
    "    \"can not\"  # sometimes appears split\n",
    "}\n",
    "\n",
    "KEEP_SHORT = {\"ok\",\"ugh\",\"wtf\",\"meh\",\"yay\",\"lol\",\"omg\"}\n",
    "KEEP_INTENSIFIERS = {\"very\",\"so\",\"too\",\"really\",\"extremely\",\"quite\",\"super\",\"incredibly\",\"totally\",\"absolutely\"}\n",
    "\n",
    "# Domain stopwords that we want to KEEP (they carry meaning here)\n",
    "PROTECTED_STOPWORDS = {\n",
    "    \"down\",   # wifi down (negative)\n",
    "    \"out\",    # system out / sold out\n",
    "    \"off\",    # turned off / took off\n",
    "    \"up\",     # back up / upgrade\n",
    "    \"back\",   # got back / baggage back\n",
    "    \"over\"    # overbooked\n",
    "}\n",
    "\n",
    "base_stop = set(stopwords.words('english'))\n",
    "# keep negations, intensifiers, and protected domain words\n",
    "custom_stop = {\n",
    "    w for w in base_stop\n",
    "    if (w not in NEGATIONS)\n",
    "    and (w not in KEEP_INTENSIFIERS)\n",
    "    and (w not in PROTECTED_STOPWORDS)\n",
    "}\n",
    "\n",
    "def _to_wordnet_pos(tb_tag):\n",
    "    if tb_tag.startswith('J'): return wordnet.ADJ\n",
    "    if tb_tag.startswith('V'): return wordnet.VERB\n",
    "    if tb_tag.startswith('N'): return wordnet.NOUN\n",
    "    if tb_tag.startswith('R'): return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # 1) normalize & basic noise removal (preserve tone signals)\n",
    "    text = contractions.fix(str(text))                         # \"don't\" -> \"do not\"\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))        # ðŸ™‚ -> slightly_smiling_face\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)             # remove URLs\n",
    "    text = re.sub(r'@\\w+', ' ', text)                         # remove mentions\n",
    "    text = text.replace('#', ' ')                             # keep hashtag token\n",
    "    # keep letters, underscores, spaces, and sentiment punctuation ! ?\n",
    "    text = re.sub(r'[^a-z_!\\?\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def protect_negations(text):\n",
    "    \"\"\"\n",
    "    Glue common (expanded + contracted) negations to the next token:\n",
    "    e.g., 'do not like' -> 'do_not_like', 'won't board' -> 'won't_board'\n",
    "    \"\"\"\n",
    "    neg_re = (\n",
    "        r\"(?:no|nor|not|never|cannot|cant|can't|\"\n",
    "        r\"do not|does not|did not|is not|are not|was not|were not|\"\n",
    "        r\"will not|won't|wont|would not|should not|could not|\"\n",
    "        r\"have not|has not|had not|can not)\"\n",
    "    )\n",
    "    # replace spaces inside [NEGATION + next_token] span with underscores\n",
    "    def repl(m): return m.group(0).replace(' ', '_')\n",
    "    return re.sub(rf\"\\b{neg_re}\\s+[a-z_]+\", repl, text)\n",
    "\n",
    "def pos_lemmatize_with_stop(text):\n",
    "    \"\"\"\n",
    "    Keep negations/emoji tokens; keep intensifiers and short emotion tokens;\n",
    "    lemmatize nouns/verbs; avoid lemmatizing ADJ/ADV to preserve tone.\n",
    "    \"\"\"\n",
    "    toks = word_tokenize(text)\n",
    "    kept = []\n",
    "    for t in toks:\n",
    "        if t in NEGATIONS or '_' in t:                  # glued negations & emoji words\n",
    "            kept.append(t)\n",
    "        elif t in KEEP_SHORT or t in KEEP_INTENSIFIERS:\n",
    "            kept.append(t)\n",
    "        elif t in custom_stop:\n",
    "            continue\n",
    "        else:\n",
    "            kept.append(t)\n",
    "\n",
    "    tagged = pos_tag(kept)\n",
    "    lemmas = []\n",
    "    for w, tag in tagged:\n",
    "        if tag.startswith('J') or tag.startswith('R'):  # keep adjectives/adverbs as-is\n",
    "            lemmas.append(w)\n",
    "        else:\n",
    "            lemmas.append(lemmatizer.lemmatize(w, _to_wordnet_pos(tag)))\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def clean_pipeline(text):\n",
    "    text = clean_tweet(text)\n",
    "    text = protect_negations(text)\n",
    "    text = pos_lemmatize_with_stop(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d69854c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()\n",
    "df_cleaned['clean_text'] = df_cleaned['text'].apply(clean_pipeline)\n",
    "df_cleaned.to_csv('cleaned_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1513c1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus added commercial experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>did_not_today must mean need take another trip !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>seriously would pay flight seat did_not_have p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>yes nearly every time fly vx ear worm will_not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>really missed prime opportunity men without ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.6559</td>\n",
       "      <td>@virginamerica Well, I didn'tâ€¦but NOW I DO! :-D</td>\n",
       "      <td>well did_not_but !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>amaze arrive hour early too good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  sentiment_confidence  \\\n",
       "0           neutral                1.0000   \n",
       "1          positive                0.3486   \n",
       "2           neutral                0.6837   \n",
       "3          negative                1.0000   \n",
       "4          negative                1.0000   \n",
       "5          negative                1.0000   \n",
       "6          positive                0.6745   \n",
       "7           neutral                0.6340   \n",
       "8          positive                0.6559   \n",
       "9          positive                1.0000   \n",
       "\n",
       "                                                text  \\\n",
       "0                @VirginAmerica What @dhepburn said.   \n",
       "1  @VirginAmerica plus you've added commercials t...   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3  @VirginAmerica it's really aggressive to blast...   \n",
       "4  @VirginAmerica and it's a really big bad thing...   \n",
       "5  @VirginAmerica seriously would pay $30 a fligh...   \n",
       "6  @VirginAmerica yes, nearly every time I fly VX...   \n",
       "7  @VirginAmerica Really missed a prime opportuni...   \n",
       "8    @virginamerica Well, I didn'tâ€¦but NOW I DO! :-D   \n",
       "9  @VirginAmerica it was amazing, and arrived an ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0                                                say  \n",
       "1             plus added commercial experience tacky  \n",
       "2   did_not_today must mean need take another trip !  \n",
       "3  really aggressive blast obnoxious entertainmen...  \n",
       "4                               really big bad thing  \n",
       "5  seriously would pay flight seat did_not_have p...  \n",
       "6  yes nearly every time fly vx ear worm will_not...  \n",
       "7  really missed prime opportunity men without ha...  \n",
       "8                                 well did_not_but !  \n",
       "9                   amaze arrive hour early too good  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c435a82",
   "metadata": {},
   "source": [
    "# Minimally Cleaned Data\n",
    "## (GloVe, SBERT, BERTweet, CardiffNLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12a5ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_clean_pipeline(text):\n",
    "    text = html.unescape(text)             # Decode &amp;, &gt;, etc.\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)    # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)       # Remove mentions\n",
    "    text = re.sub(r\"\\s+\", \" \", text)       # Normalize whitespace\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeb224dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_min_cleaned = df.copy()\n",
    "df_min_cleaned['clean_text'] = df_min_cleaned['text'].apply(min_clean_pipeline)\n",
    "df_min_cleaned.to_csv('min_cleaned_tweets.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a1b25b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>What said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus you've added commercials to the experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>I didn't today... Must mean I need to take ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>it's really aggressive to blast obnoxious \"ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>seriously would pay $30 a flight for seats tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>yes, nearly every time I fly VX this â€œear worm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>Really missed a prime opportunity for Men With...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>0.6559</td>\n",
       "      <td>@virginamerica Well, I didn'tâ€¦but NOW I DO! :-D</td>\n",
       "      <td>Well, I didn'tâ€¦but NOW I DO! :-D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>it was amazing, and arrived an hour early. You...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  sentiment_confidence  \\\n",
       "0           neutral                1.0000   \n",
       "1          positive                0.3486   \n",
       "2           neutral                0.6837   \n",
       "3          negative                1.0000   \n",
       "4          negative                1.0000   \n",
       "5          negative                1.0000   \n",
       "6          positive                0.6745   \n",
       "7           neutral                0.6340   \n",
       "8          positive                0.6559   \n",
       "9          positive                1.0000   \n",
       "\n",
       "                                                text  \\\n",
       "0                @VirginAmerica What @dhepburn said.   \n",
       "1  @VirginAmerica plus you've added commercials t...   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3  @VirginAmerica it's really aggressive to blast...   \n",
       "4  @VirginAmerica and it's a really big bad thing...   \n",
       "5  @VirginAmerica seriously would pay $30 a fligh...   \n",
       "6  @VirginAmerica yes, nearly every time I fly VX...   \n",
       "7  @VirginAmerica Really missed a prime opportuni...   \n",
       "8    @virginamerica Well, I didn'tâ€¦but NOW I DO! :-D   \n",
       "9  @VirginAmerica it was amazing, and arrived an ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0                                         What said.  \n",
       "1  plus you've added commercials to the experienc...  \n",
       "2  I didn't today... Must mean I need to take ano...  \n",
       "3  it's really aggressive to blast obnoxious \"ent...  \n",
       "4           and it's a really big bad thing about it  \n",
       "5  seriously would pay $30 a flight for seats tha...  \n",
       "6  yes, nearly every time I fly VX this â€œear worm...  \n",
       "7  Really missed a prime opportunity for Men With...  \n",
       "8                   Well, I didn'tâ€¦but NOW I DO! :-D  \n",
       "9  it was amazing, and arrived an hour early. You...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_min_cleaned.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
